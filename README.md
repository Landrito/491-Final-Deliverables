When first starting this class, Evan West and I wanted to implement the Q-Learning convolutional neural net of Deep Mind. This plan did not end up working out because deepmind released the atari code. We then embarked on trying to add something to the atari code to try and improve upon deep minds code. For a fair amount of time we worked on trying to make an A* algorithm that would complement the Qlearning neural network. After working on this, we found that the algorithm would be far too complex because in order to find the shortest path, we would have to somehow produce a large amount of possible game staStes that can be derived from the current game state. This task alone made the algorithm both too complex and also takes away from the core idea of deep mind's original work, in which a computer can learn a game simply from the game screen and positive feedback from the game. A* was simply too specific. 

After realizing that A* was not the appropriate tool, Evan and I discussed implementing LSTM into the deep mind code and decided to do that instead. Over the course, Evan had major family emergencies which made him very hard to meet up with and work with, and because of the family emergencies, Evan had to request an incomplete grade. Working alone, 

I tried to implement LSTM into the code. I used the rnn library to try and plug the system in. At first I thought putting LSTM into the code would be as simple as adding the LSTM module after the convolutional layers and before the softmax layer, so I tried that. This did not work because the recurrency found inside an LSTM memory cell caused it to not be allowed to be inserted into the sequential container that the qlearning code was using. After seeing that problem, I found that you could insert recurrent modules into a sequential by using a sequencer. This allowed the neural net to forward propogate but when the system would reach the backpropagation and optimization of the minibatches, the LSTM cell proved to be incompatable to system that deepmind wrote because all of the other modules when backpropagated expected a different type than that of the LSTM cell. I worked on this problem extensively with no avail from the time of Evans departure.

After seeing that LSTM was not going to be ready in time, I thought I might try one of the simpler solutions to making deepmind's code better. In deepminds code, when learning how to play breakout, the reward for the system is 1 for hitting a break at the current step, or 0 for not hitting the brick at the current time step. This system made it so that the rewards came few and far between. Furthermore, the system would learn a way to only get 48 out of the 50 bricks. This learning of how to play breakout took over two days on a GPU to get to human like playthroughs. I thought what would be a more intuitive way to play the game would be not only to reward the agent for hitting a brick, but also to reward the player greatly for hitting a brick in the shortest amount of time. This idea stemmed from the thought that the qlearning system should not only learn how to play the game, but it should learn how to beat the game in the fastest time. In order to bring this intuition to life, I added what I call a time punishment to the qlearning algorithm. This time punishment is a scalar that is multiplied by the number of time steps since the last reward that was given. I expected that this method would make the system learn faster and be able to finish the game. 

I ran this code for one day on the CPU (my hardware is pretty bad for running neural nets) which equated to about 2900 games played by the system. These games were measured by how many points scored per game. After running the code, I found that the punishment value did not help the code run faster. In teh picture provided, it can be see that the system was not learning very fast as the amount of points scored tended to stay remotely the same as when it started. Furthermore, this can be seen through the average points scored per game staying roughly the same at 1.34 points per game. Because of the data found, it can be concluded that by adding a time scaled reward punishment, the system will not be improved.
